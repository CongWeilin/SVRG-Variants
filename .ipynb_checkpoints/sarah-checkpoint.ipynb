{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LR_SCSG = 0.1\n",
    "\n",
    "EPOCH = 20\n",
    "LARGE_BATCH_NUMBER=25\n",
    "INNER_LOOPS = 10\n",
    "\n",
    "# Setup DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        x = self.model(inputs)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "    \n",
    "    def partial_grad(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Function to compute the stochastic gradient\n",
    "        args : input, loss_function\n",
    "        return : loss\n",
    "        \"\"\"\n",
    "        outputs = self.forward(inputs)\n",
    "        # compute the partial loss\n",
    "        loss = F.nll_loss(outputs, targets)\n",
    "\n",
    "        # compute gradient\n",
    "        loss.backward()\n",
    "        return loss.detach()\n",
    "    \n",
    "    def calculate_loss_grad(self, dataset, large_batch_num):\n",
    "        \"\"\"\n",
    "        Function to compute the large-batch loss and the large-batch gradient\n",
    "        args : dataset, loss function, number of samples to be calculated\n",
    "        return : total_loss, full_grad_norm\n",
    "        \"\"\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for idx, data in enumerate(dataset):\n",
    "            # only calculate the sub-sampled large batch\n",
    "            if idx > large_batch_num - 1:\n",
    "                break\n",
    "            # load input\n",
    "            inputs, targets = data\n",
    "            inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "            # calculate loss\n",
    "            total_loss += self.partial_grad(inputs, targets)\n",
    "            \n",
    "        total_loss /= large_batch_num\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_variance(net, train_loader, large_batch_num):\n",
    "    net_grads = []\n",
    "    for p_net in net.parameters():\n",
    "        net_grads.append(p_net.grad.data)\n",
    "    clone_net = copy.deepcopy(net)\n",
    "    _ = clone_net.calculate_loss_grad(train_loader, large_batch_num)\n",
    "\n",
    "    clone_net_grad = []\n",
    "    for p_net in clone_net.parameters():\n",
    "        clone_net_grad.append(p_net.grad.data/large_batch_num)\n",
    "    del clone_net\n",
    "    \n",
    "    variance = 0.0\n",
    "    for g1, g2 in zip(net_grads, clone_net_grad):\n",
    "        variance += (g1-g2).norm(2) ** 2\n",
    "    variance = torch.sqrt(variance)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarah_step(net, optimizer, train_loader, test_loader, inner_iter_num):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SARAH backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    large_batch_num = LARGE_BATCH_NUMBER\n",
    "    batch_size = BATCH_SIZE\n",
    "    \n",
    "    # record previous net full gradient\n",
    "    pre_net_full = copy.deepcopy(net)\n",
    "    # record previous net mini batch gradient\n",
    "    pre_net_mini = copy.deepcopy(net)\n",
    "\n",
    "    # Compute full grad\n",
    "    optimizer.zero_grad()\n",
    "    _ = net.calculate_loss_grad(train_loader, large_batch_num)\n",
    "\n",
    "    for p_net in net.parameters():\n",
    "        p_net.grad.data *= (1.0 / large_batch_num)\n",
    "        \n",
    "    optimizer.step()\n",
    "        \n",
    "    running_loss = []\n",
    "    iter_num = 0.0\n",
    "    grad_variances = []\n",
    "    # Run over the train_loader\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "\n",
    "        if batch_id > inner_iter_num - 1:\n",
    "            break\n",
    "        \n",
    "        # record previous net full gradient\n",
    "        for p_net, p_full in zip(net.parameters(), pre_net_full.parameters()):\n",
    "            p_full.grad = copy.deepcopy(p_net.grad)\n",
    "        \n",
    "        # get the input and label\n",
    "        inputs, targets = batch_data\n",
    "\n",
    "        # wrap data and target into variable\n",
    "        inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "        # compute previous stochastic gradient\n",
    "        pre_net_mini.zero_grad()\n",
    "        # take backward\n",
    "        _ = pre_net_mini.partial_grad(inputs, targets)\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "        current_loss = net.partial_grad(inputs, targets)\n",
    "\n",
    "        # take SCSG gradient step\n",
    "        for p_net, p_mini, p_full in zip(net.parameters(), pre_net_mini.parameters(), pre_net_full.parameters()):\n",
    "            p_net.grad.data = p_net.grad.data + p_full.grad.data - p_mini.grad.data\n",
    "        \n",
    "        grad_variances += [calculate_grad_variance(net, train_loader, LARGE_BATCH_NUMBER)]\n",
    "        \n",
    "        # record previous net mini batch gradient\n",
    "        for p_mini, p_net in zip(pre_net_mini.parameters(), net.parameters()):\n",
    "            p_mini.data = copy.deepcopy(p_net.data)\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += [current_loss.cpu().detach()]\n",
    "        iter_num += 1.0\n",
    "\n",
    "    # calculate training loss\n",
    "    train_loss = np.mean(running_loss)\n",
    "\n",
    "    # calculate test loss\n",
    "    net.zero_grad()\n",
    "    test_loss = net.calculate_loss_grad(test_loader, len(test_loader)/batch_size)\n",
    "\n",
    "    return train_loss, test_loss, running_loss, grad_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(net, optimizer, train_loader, test_loader, inner_iter_num):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SGD backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    batch_size = BATCH_SIZE\n",
    "\n",
    "    running_loss = []\n",
    "    iter_num = 0.0\n",
    "    grad_variances = []\n",
    "    \n",
    "    # Run over the train_loader\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "\n",
    "        if batch_id > inner_iter_num - 1:\n",
    "            break\n",
    "\n",
    "        # get the input and label\n",
    "        inputs, targets = batch_data\n",
    "\n",
    "        # wrap data and target into variable\n",
    "        inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        current_loss = F.nll_loss(outputs, targets)\n",
    "        current_loss.backward()\n",
    "        \n",
    "        grad_variances.append(calculate_grad_variance(net, train_loader, LARGE_BATCH_NUMBER))\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += [current_loss.cpu().detach()]\n",
    "        iter_num += 1.0\n",
    "\n",
    "    # calculate training loss\n",
    "    train_loss = np.mean(running_loss)\n",
    "\n",
    "    # calculate test loss\n",
    "    net.zero_grad()\n",
    "    test_loss = net.calculate_loss_grad(test_loader, len(test_loader)/batch_size)\n",
    "\n",
    "    return train_loss, test_loss, running_loss, grad_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 1.94072378 | test loss: 554.52423096\n",
      "Epoch:  1 | train loss: nan | test loss: nan\n",
      "Epoch:  2 | train loss: nan | test loss: nan\n",
      "Epoch:  3 | train loss: nan | test loss: nan\n",
      "Epoch:  4 | train loss: nan | test loss: nan\n",
      "Epoch:  5 | train loss: nan | test loss: nan\n",
      "Epoch:  6 | train loss: nan | test loss: nan\n",
      "Epoch:  7 | train loss: nan | test loss: nan\n",
      "Epoch:  8 | train loss: nan | test loss: nan\n",
      "Epoch:  9 | train loss: nan | test loss: nan\n",
      "Epoch:  10 | train loss: nan | test loss: nan\n",
      "Epoch:  11 | train loss: nan | test loss: nan\n",
      "Epoch:  12 | train loss: nan | test loss: nan\n",
      "Epoch:  13 | train loss: nan | test loss: nan\n",
      "Epoch:  14 | train loss: nan | test loss: nan\n",
      "Epoch:  15 | train loss: nan | test loss: nan\n",
      "Epoch:  16 | train loss: nan | test loss: nan\n",
      "Epoch:  17 | train loss: nan | test loss: nan\n",
      "Epoch:  18 | train loss: nan | test loss: nan\n",
      "Epoch:  19 | train loss: nan | test loss: nan\n"
     ]
    }
   ],
   "source": [
    "net = Model().cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LR_SCSG)\n",
    "# optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "sarah_loss_train = []\n",
    "sarah_loss_test  = []\n",
    "sarah_loss_all = []\n",
    "sarah_grad_variances = []\n",
    "# training\n",
    "for epoch in range(EPOCH):\n",
    "    inner_iter_num = INNER_LOOPS #np.random.geometric(1.0/(LARGE_BATCH_NUMBER + 1.0))\n",
    "    # take one epoch scsg step\n",
    "    cur_train_loss, cur_test_loss, cur_loss_all, cur_grad_variances\\\n",
    "        = sarah_step(net, optimizer, train_loader, test_loader, inner_iter_num)\n",
    "    sarah_loss_train.append(cur_train_loss)\n",
    "    sarah_loss_test.append(cur_test_loss)\n",
    "    sarah_loss_all.extend(cur_loss_all)\n",
    "    sarah_grad_variances.extend(cur_grad_variances)\n",
    "    # print progress\n",
    "    print('Epoch: ', epoch,\n",
    "          '| train loss: %.8f' % cur_train_loss,\n",
    "          '| test loss: %.8f' % cur_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.80626917 | test loss: 2859.34448242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-917511460ffe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# take one epoch scsg step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcur_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_test_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_loss_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_grad_variances\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;34m=\u001b[0m \u001b[0msgd_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_iter_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0msgd_loss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_train_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0msgd_loss_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_test_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-246fe4405613>\u001b[0m in \u001b[0;36msgd_step\u001b[0;34m(net, optimizer, train_loader, test_loader, inner_iter_num)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcurrent_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mgrad_variances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalculate_grad_variance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLARGE_BATCH_NUMBER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0d3c4a9e990c>\u001b[0m in \u001b[0;36mcalculate_grad_variance\u001b[0;34m(net, train_loader, large_batch_num)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mnet_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclone_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_loss_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlarge_batch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclone_net_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-c0dda5218d68>\u001b[0m in \u001b[0;36mcalculate_loss_grad\u001b[0;34m(self, dataset, large_batch_num)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlarge_batch_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = Model().cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LR_SCSG)\n",
    "# optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "sgd_loss_train = []\n",
    "sgd_loss_test  = []\n",
    "sgd_loss_all = []\n",
    "sgd_grad_variances = []\n",
    "# training\n",
    "for epoch in range(EPOCH):\n",
    "    inner_iter_num = INNER_LOOPS # len(train_loader)/BATCH_SIZE\n",
    "    # take one epoch scsg step\n",
    "    cur_train_loss, cur_test_loss, cur_loss_all, cur_grad_variances\\\n",
    "        = sgd_step(net, optimizer, train_loader, test_loader, inner_iter_num)\n",
    "    sgd_loss_train.append(cur_train_loss)\n",
    "    sgd_loss_test.append(cur_test_loss)\n",
    "    sgd_loss_all.extend(cur_loss_all)\n",
    "    sgd_grad_variances.extend(cur_grad_variances)\n",
    "    # print progress\n",
    "    print('Epoch: ', epoch,\n",
    "          '| train loss: %.8f' % cur_train_loss,\n",
    "          '| test loss: %.8f' % cur_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_loss_train,label='sgd_loss_train')\n",
    "axs.plot(x,sarah_loss_train,label='sarah_loss_train')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('train_loss_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_loss_test,label='sgd_loss_test')\n",
    "axs.plot(x,sarah_loss_test,label='sarah_loss_test')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('test_loss_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_loss_all[:EPOCH],label='sgd_loss_all')\n",
    "axs.plot(x,sarah_loss_all[:EPOCH],label='sarah_loss_all')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('all_loss_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_grad_variances[:EPOCH],label='sgd_grad_variances')\n",
    "axs.plot(x,sarah_grad_variances[:EPOCH],label='sarah_grad_variances')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('grad_variances_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
