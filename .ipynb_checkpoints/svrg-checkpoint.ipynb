{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "LR_SCSG = 0.06\n",
    "\n",
    "EPOCH = 50\n",
    "LARGE_BATCH_NUMBER=25\n",
    "INNER_LOOPS = 10\n",
    "\n",
    "# Setup DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.Linear(512, 10))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs = inputs.view(-1, 28 * 28)\n",
    "        x = self.model(inputs)\n",
    "        return F.log_softmax(x,dim=1)\n",
    "    \n",
    "    def partial_grad(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Function to compute the stochastic gradient\n",
    "        args : input, loss_function\n",
    "        return : loss\n",
    "        \"\"\"\n",
    "        outputs = self.forward(inputs)\n",
    "        # compute the partial loss\n",
    "        loss = F.nll_loss(outputs, targets)\n",
    "\n",
    "        # compute gradient\n",
    "        loss.backward()\n",
    "        return loss.detach()\n",
    "    \n",
    "    def calculate_loss_grad(self, dataset, large_batch_num):\n",
    "        \"\"\"\n",
    "        Function to compute the large-batch loss and the large-batch gradient\n",
    "        args : dataset, loss function, number of samples to be calculated\n",
    "        return : total_loss, full_grad_norm\n",
    "        \"\"\"\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for idx, data in enumerate(dataset):\n",
    "            # only calculate the sub-sampled large batch\n",
    "            if idx > large_batch_num - 1:\n",
    "                break\n",
    "            # load input\n",
    "            inputs, targets = data\n",
    "            inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "            # calculate loss\n",
    "            total_loss += self.partial_grad(inputs, targets)\n",
    "            \n",
    "        total_loss /= large_batch_num\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grad_variance(net, train_loader, large_batch_num):\n",
    "    net_grads = []\n",
    "    for p_net in net.parameters():\n",
    "        net_grads.append(p_net.grad.data)\n",
    "    clone_net = copy.deepcopy(net)\n",
    "    _ = clone_net.calculate_loss_grad(train_loader, large_batch_num)\n",
    "\n",
    "    clone_net_grad = []\n",
    "    for p_net in clone_net.parameters():\n",
    "        clone_net_grad.append(p_net.grad.data/large_batch_num)\n",
    "    del clone_net\n",
    "    \n",
    "    variance = 0.0\n",
    "    for g1, g2 in zip(net_grads, clone_net_grad):\n",
    "        variance += (g1-g2).norm(2) ** 2\n",
    "    variance = torch.sqrt(variance)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scsg_step(net, optimizer, train_loader, test_loader, inner_iter_num):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SCSG backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    # record previous net full gradient\n",
    "    pre_net_full = copy.deepcopy(net)\n",
    "    # record previous net mini batch gradient\n",
    "    pre_net_mini = copy.deepcopy(net)\n",
    "\n",
    "    large_batch_num = LARGE_BATCH_NUMBER\n",
    "    batch_size = BATCH_SIZE\n",
    "\n",
    "    # Compute full grad\n",
    "    pre_net_full.zero_grad()\n",
    "    _ = pre_net_full.calculate_loss_grad(train_loader, large_batch_num)\n",
    "\n",
    "    running_loss = []\n",
    "    iter_num = 0.0\n",
    "    grad_variances = []\n",
    "\n",
    "    # Run over the train_loader\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "\n",
    "        if batch_id > inner_iter_num - 1:\n",
    "            break\n",
    "\n",
    "        # get the input and label\n",
    "        inputs, targets = batch_data\n",
    "\n",
    "        # wrap data and target into variable\n",
    "        inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "        # compute previous stochastic gradient\n",
    "        pre_net_mini.zero_grad()\n",
    "        # take backward\n",
    "        pre_net_mini.partial_grad(inputs, targets)\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        current_loss = F.nll_loss(outputs, targets)\n",
    "        current_loss.backward()\n",
    "\n",
    "        # take SCSG gradient step\n",
    "        for p_net, p_mini, p_full in zip(net.parameters(), pre_net_mini.parameters(), pre_net_full.parameters()):\n",
    "            p_net.grad.data += p_full.grad.data * (1.0 / large_batch_num) - p_mini.grad.data\n",
    "            \n",
    "        grad_variances += [calculate_grad_variance(net, train_loader, LARGE_BATCH_NUMBER)]\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += [current_loss.cpu().detach()]\n",
    "        iter_num += 1.0\n",
    "\n",
    "    # calculate training loss\n",
    "    train_loss = np.mean(running_loss)\n",
    "\n",
    "    # calculate test loss\n",
    "    net.zero_grad()\n",
    "    test_loss = net.calculate_loss_grad(test_loader, len(test_loader)/batch_size)\n",
    "\n",
    "    return train_loss, test_loss, grad_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarah_step(net, optimizer, train_loader, test_loader, inner_iter_num):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SARAH backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    large_batch_num = LARGE_BATCH_NUMBER\n",
    "    batch_size = BATCH_SIZE\n",
    "    \n",
    "    # record previous net mini batch gradient\n",
    "    pre_net_mini = copy.deepcopy(net)\n",
    "\n",
    "    # Compute full grad\n",
    "    optimizer.zero_grad()\n",
    "    _ = net.calculate_loss_grad(train_loader, large_batch_num)\n",
    "\n",
    "    for p_net in net.parameters():\n",
    "        p_net.grad.data *= (1.0 / large_batch_num)\n",
    "    \n",
    "    # record previous net full gradient\n",
    "    pre_net_full = []\n",
    "    for p_net in net.parameters():\n",
    "        pre_net_full.append(p_net.grad.data)\n",
    "        \n",
    "    # torch.nn.utils.clip_grad_norm_(net.parameters(), 0.2)\n",
    "    optimizer.step()\n",
    "\n",
    "    running_loss = []\n",
    "    iter_num = 0.0\n",
    "    grad_variances = []\n",
    "    # Run over the train_loader\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "\n",
    "        if batch_id > inner_iter_num - 1:\n",
    "            break\n",
    "        \n",
    "        # get the input and label\n",
    "        inputs, targets = batch_data\n",
    "\n",
    "        # wrap data and target into variable\n",
    "        inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "        # compute previous stochastic gradient\n",
    "        pre_net_mini.zero_grad()\n",
    "        # take backward\n",
    "        pre_net_mini.partial_grad(inputs, targets)\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        current_loss = F.nll_loss(outputs, targets)\n",
    "        current_loss.backward()\n",
    "\n",
    "        # take SCSG gradient step\n",
    "        for p_net, p_mini, p_full in zip(net.parameters(), pre_net_mini.parameters(), pre_net_full):\n",
    "            p_net.grad.data += p_full - p_mini.grad.data\n",
    "        \n",
    "        grad_variances += [calculate_grad_variance(net, train_loader, LARGE_BATCH_NUMBER)]\n",
    "        # record previous net full gradient\n",
    "        pre_net_full = []\n",
    "        for p_net in net.parameters():\n",
    "            pre_net_full.append(p_net.grad.data)\n",
    "        \n",
    "        # record previous net mini batch gradient\n",
    "        del pre_net_mini\n",
    "        pre_net_mini = copy.deepcopy(net)\n",
    "        \n",
    "        # torch.nn.utils.clip_grad_norm_(net.parameters(), 0.2)\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += [current_loss.cpu().detach()]\n",
    "        iter_num += 1.0\n",
    "\n",
    "    # calculate training loss\n",
    "    train_loss = np.mean(running_loss)\n",
    "\n",
    "    # calculate test loss\n",
    "    net.zero_grad()\n",
    "    test_loss = net.calculate_loss_grad(test_loader, len(test_loader)/batch_size)\n",
    "\n",
    "    return train_loss, test_loss, running_loss, grad_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_step(net, optimizer, train_loader, test_loader, inner_iter_num):\n",
    "    \"\"\"\n",
    "    Function to updated weights with a SGD backpropagation\n",
    "    args : net, optimizer, train_loader, test_loader, loss function, number of inner epochs, args\n",
    "    return : train_loss, test_loss, grad_norm_lb\n",
    "    \"\"\"\n",
    "    batch_size = BATCH_SIZE\n",
    "\n",
    "    running_loss = []\n",
    "    iter_num = 0.0\n",
    "    grad_variances = []\n",
    "    \n",
    "    # Run over the train_loader\n",
    "    for batch_id, batch_data in enumerate(train_loader):\n",
    "\n",
    "        if batch_id > inner_iter_num - 1:\n",
    "            break\n",
    "\n",
    "        # get the input and label\n",
    "        inputs, targets = batch_data\n",
    "\n",
    "        # wrap data and target into variable\n",
    "        inputs, targets = torch.FloatTensor(inputs).cuda(), torch.LongTensor(targets).cuda()\n",
    "\n",
    "        # compute current stochastic gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        current_loss = F.nll_loss(outputs, targets)\n",
    "        current_loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(net.parameters(), 0.2)\n",
    "        \n",
    "        grad_variances.append(calculate_grad_variance(net, train_loader, LARGE_BATCH_NUMBER))\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += [current_loss.cpu().detach()]\n",
    "        iter_num += 1.0\n",
    "\n",
    "    # calculate training loss\n",
    "    train_loss = np.mean(running_loss)\n",
    "\n",
    "    # calculate test loss\n",
    "    net.zero_grad()\n",
    "    test_loss = net.calculate_loss_grad(test_loader, len(test_loader)/batch_size)\n",
    "\n",
    "    return train_loss, test_loss, running_loss, grad_variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 2.26240802 | test loss: 2.19919229\n",
      "Epoch:  1 | train loss: 2.15750122 | test loss: 2.08887720\n",
      "Epoch:  2 | train loss: 2.04071593 | test loss: 1.90675902\n",
      "Epoch:  3 | train loss: 1.83213139 | test loss: 1.70942903\n",
      "Epoch:  4 | train loss: 1.55498695 | test loss: 1.30191267\n",
      "Epoch:  5 | train loss: 1.23092151 | test loss: 1.00937843\n"
     ]
    }
   ],
   "source": [
    "net = Model().cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LR_SCSG)\n",
    "# optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "sarah_loss_train = []\n",
    "sarah_loss_test  = []\n",
    "sarah_loss_all = []\n",
    "sarah_grad_variances = []\n",
    "# training\n",
    "for epoch in range(EPOCH):\n",
    "    inner_iter_num = INNER_LOOPS #np.random.geometric(1.0/(LARGE_BATCH_NUMBER + 1.0))\n",
    "    # take one epoch scsg step\n",
    "    cur_train_loss, cur_test_loss, cur_loss_all, cur_grad_variances\\\n",
    "        = sarah_step(net, optimizer, train_loader, test_loader, inner_iter_num)\n",
    "    sarah_loss_train.append(cur_train_loss)\n",
    "    sarah_loss_test.append(cur_test_loss)\n",
    "    sarah_loss_all.extend(cur_loss_all)\n",
    "    sarah_grad_variances.extend(cur_grad_variances)\n",
    "    # print progress\n",
    "    print('Epoch: ', epoch,\n",
    "          '| train loss: %.8f' % cur_train_loss,\n",
    "          '| test loss: %.8f' % cur_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Model().cuda()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=LR_SCSG)\n",
    "# optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "sgd_loss_train = []\n",
    "sgd_loss_test  = []\n",
    "sgd_loss_all = []\n",
    "sgd_grad_variances = []\n",
    "# training\n",
    "for epoch in range(EPOCH):\n",
    "    inner_iter_num = INNER_LOOPS # len(train_loader)/BATCH_SIZE\n",
    "    # take one epoch scsg step\n",
    "    cur_train_loss, cur_test_loss, cur_loss_all, cur_grad_variances\\\n",
    "        = sgd_step(net, optimizer, train_loader, test_loader, inner_iter_num)\n",
    "    sgd_loss_train.append(cur_train_loss)\n",
    "    sgd_loss_test.append(cur_test_loss)\n",
    "    sgd_loss_all.extend(cur_loss_all)\n",
    "    sgd_grad_variances.extend(cur_grad_variances)\n",
    "    # print progress\n",
    "    print('Epoch: ', epoch,\n",
    "          '| train loss: %.8f' % cur_train_loss,\n",
    "          '| test loss: %.8f' % cur_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_loss_train,label='sgd_loss_train')\n",
    "axs.plot(x,sarah_loss_train,label='sarah_loss_train')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('train_loss_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_loss_test,label='sgd_loss_test')\n",
    "axs.plot(x,sarah_loss_test,label='sarah_loss_test')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('test_loss_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_loss_all[:EPOCH],label='sgd_loss_all')\n",
    "axs.plot(x,sarah_loss_all[:EPOCH],label='sarah_loss_all')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('all_loss_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "x = np.arange(EPOCH)\n",
    "axs.plot(x,sgd_grad_variances[:EPOCH],label='sgd_grad_variances')\n",
    "axs.plot(x,sarah_grad_variances[:EPOCH],label='sarah_grad_variances')\n",
    "        \n",
    "axs.set_xlabel('iters')\n",
    "axs.set_ylabel('loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('grad_variances_sarah.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
